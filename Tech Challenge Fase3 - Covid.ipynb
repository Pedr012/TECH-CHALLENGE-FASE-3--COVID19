{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110a6b3f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a861a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import sys\n",
    "import time \n",
    "\n",
    "# Importar algo especifico de uma biblioteca\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097aa01e",
   "metadata": {},
   "source": [
    "### Testar a conex√£o de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfca80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conex√£o ao banco de dados\n",
    "def test_connection(engine):\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Testar a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Listar as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"-\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d1325",
   "metadata": {},
   "source": [
    "### Vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do bucket S3 e subpastas\n",
    "s3_bucket = 'postech-covid19-gp81'\n",
    "s3_bronze = 'bronze'\n",
    "s3_silver = 'silver'\n",
    "s3_gold = 'gold'\n",
    "\n",
    "silver_prefix = f's3://{s3_bucket}/{s3_silver}/'\n",
    "gold_prefix = f's3://{s3_bucket}/{s3_gold}/'\n",
    "\n",
    "# Caminho do Github com dados do PNAD\n",
    "api_url = 'https://api.github.com/repos/Pedr012/TECH-CHALLENGE-FASE-3---COVID19/contents/BASE_PNAD_COVID'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a727b",
   "metadata": {},
   "source": [
    "### Configura√ß√£o AWS e Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e520006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar as credencias do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configura√ß√£o storage_options\n",
    "storage_options = {\n",
    "    \"key\": os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    \"secret\": os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    \"token\": os.getenv('AWS_SESSION_TOKEN')\n",
    "}\n",
    "\n",
    "# Configura√ß√£o S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
    "    region_name=os.getenv('AWS_REGION')\n",
    ")\n",
    "\n",
    "# Credenciais do PostgreSQL\n",
    "usuario_pg = os.getenv(\"POSTGRES_USER\")\n",
    "senha_pg = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host_pg = os.getenv(\"POSTGRES_HOST\")\n",
    "porta_pg = os.getenv(\"POSTGRES_PORT\")\n",
    "banco_pg = os.getenv(\"POSTGRES_DATABASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f596da",
   "metadata": {},
   "source": [
    "### Validar e testar as conex√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080ec3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado √† conta\n",
      "\n",
      "UserId: AROA6GBMGAURQRLE7O4BA:user1578413=Pedro_Henrique_Rocha_Farias\n",
      "Account: 975050245411\n",
      "Arn: arn:aws:sts::975050245411:assumed-role/voclabs/user1578413=Pedro_Henrique_Rocha_Farias\n"
     ]
    }
   ],
   "source": [
    "# Validar conex√£o com a AWS atrav√©s do .env\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    sts_client = boto3.client(\n",
    "        'sts',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
    "        region_name=os.getenv('AWS_REGION')\n",
    "    )\n",
    "    \n",
    "    identity = sts_client.get_caller_identity()\n",
    "    print(\"‚úÖ Conectado √† conta\\n\")\n",
    "    print(\"UserId:\", identity[\"UserId\"])\n",
    "    print(\"Account:\", identity[\"Account\"])\n",
    "    print(\"Arn:\", identity[\"Arn\"])\n",
    "\n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"‚ùå Erro ao conectar √† AWS. Verifique suas credenciais e tente novamente.\")\n",
    "    print(\"Detalhes do erro:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccdd67df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bucket 'postech-covid19-gp81' encontrado e acess√≠vel\n",
      "\n",
      "üìÅ Conte√∫do do bucket 'postech-covid19-gp81':\n",
      "  - bronze/.keep (0 bytes)\n",
      "  - bronze/PNAD_COVID_082020.csv (115229404 bytes)\n",
      "  - bronze/PNAD_COVID_092020.csv (115445896 bytes)\n",
      "  - bronze/PNAD_COVID_102020.csv (113515233 bytes)\n",
      "  - bronze/PNAD_COVID_112020.csv (115141700 bytes)\n",
      "  - gold/.keep (0 bytes)\n",
      "  - silver/.keep (0 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Verificar conex√£o com o bucket S3\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"‚úÖ Bucket '{s3_bucket}' encontrado e acess√≠vel\")\n",
    "    \n",
    "    # Listar objetos no bucket para mostrar o conte√∫do\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket)\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        print(f\"\\nüìÅ Conte√∫do do bucket '{s3_bucket}':\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(f\"\\nüìÅ Bucket '{s3_bucket}' est√° vazio\")\n",
    "        \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == '404':\n",
    "        print(f\"‚ùå Bucket '{s3_bucket}' n√£o encontrado\")\n",
    "    elif error_code == '403':\n",
    "        print(f\"‚ö†Ô∏è Bucket '{s3_bucket}' existe, mas voc√™ n√£o tem permiss√£o para acess√°-lo\")\n",
    "        print(\"Verifique suas credenciais AWS ou as pol√≠ticas de permiss√£o do bucket\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erro ao verificar bucket '{s3_bucket}': {e}\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro inesperado ao verificar bucket '{s3_bucket}': {e}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53388e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Subpasta 'bronze' j√° existe\n",
      "‚ÑπÔ∏è Subpasta 'silver' j√° existe\n",
      "‚ÑπÔ∏è Subpasta 'gold' j√° existe\n"
     ]
    }
   ],
   "source": [
    "# Criar estrutura de subpastas no bucket S3\n",
    "subpastas = [s3_bronze, s3_silver, s3_gold]\n",
    "\n",
    "for subpasta in subpastas:\n",
    "    try:\n",
    "        # Verificar se a subpasta j√° existe\n",
    "        key = f\"{subpasta}/.keep\"\n",
    "        \n",
    "        try:\n",
    "            s3_client.head_object(Bucket=s3_bucket, Key=key)\n",
    "            print(f\"‚ÑπÔ∏è Subpasta '{subpasta}' j√° existe\")\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                # Subpasta n√£o existe, criar\n",
    "                s3_client.put_object(\n",
    "                    Bucket=s3_bucket,\n",
    "                    Key=key,\n",
    "                    Body=b''\n",
    "                )\n",
    "                print(f\"‚úÖ Subpasta '{subpasta}' criada com sucesso\")\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar subpasta '{subpasta}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051395cc",
   "metadata": {},
   "source": [
    "### Ingesta dos dados na camada bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a56cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Obtendo lista de arquivos do reposit√≥rio GitHub...\n",
      "‚úÖ Encontrados 5 arquivos no reposit√≥rio\n",
      "üì¶ Encontrados 4 arquivos ZIP para ingest√£o\n",
      "üìÅ Encontrados 4 arquivos j√° existentes na camada bronze\n",
      "üîÑ Processando arquivo: PNAD_COVID_082020.zip\n",
      "üìä Encontrados 1 arquivos CSV no ZIP\n",
      "‚è≠Ô∏è PNAD_COVID_082020.csv j√° existe na camada bronze - ignorando\n",
      "üîÑ Processando arquivo: PNAD_COVID_092020.zip\n",
      "üìä Encontrados 1 arquivos CSV no ZIP\n",
      "‚è≠Ô∏è PNAD_COVID_092020.csv j√° existe na camada bronze - ignorando\n",
      "üîÑ Processando arquivo: PNAD_COVID_102020.zip\n",
      "üìä Encontrados 1 arquivos CSV no ZIP\n",
      "‚è≠Ô∏è PNAD_COVID_102020.csv j√° existe na camada bronze - ignorando\n",
      "üîÑ Processando arquivo: PNAD_COVID_112020.zip\n",
      "üìä Encontrados 1 arquivos CSV no ZIP\n",
      "‚è≠Ô∏è PNAD_COVID_112020.csv j√° existe na camada bronze - ignorando\n",
      "\n",
      "==================================================\n",
      "üìä RESUMO DA INGEST√ÉO:\n",
      "‚úÖ Bases ingeridas: 0\n",
      "‚è≠Ô∏è Bases ignoradas (j√° existentes): 4\n",
      "  üìÅ PNAD_COVID_082020.csv\n",
      "  üìÅ PNAD_COVID_092020.csv\n",
      "  üìÅ PNAD_COVID_102020.csv\n",
      "  üìÅ PNAD_COVID_112020.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Primeiro, obter a lista de arquivos da API do GitHub\n",
    "try:\n",
    "    print(\"üîÑ Obtendo lista de arquivos do reposit√≥rio GitHub...\")\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "    arquivos = response.json()\n",
    "    \n",
    "    print(f\"‚úÖ Encontrados {len(arquivos)} arquivos no reposit√≥rio\")\n",
    "    \n",
    "    # Filtrar apenas arquivos .zip\n",
    "    arquivos_zip = [arquivo for arquivo in arquivos if arquivo['name'].endswith('.zip')]\n",
    "    print(f\"üì¶ Encontrados {len(arquivos_zip)} arquivos ZIP para ingest√£o\")\n",
    "    \n",
    "    # Obter lista de arquivos j√° existentes na camada bronze\n",
    "    try:\n",
    "        response_s3 = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=f\"{s3_bronze}/\")\n",
    "        arquivos_existentes = []\n",
    "        if 'Contents' in response_s3:\n",
    "            arquivos_existentes = [obj['Key'].replace(f\"{s3_bronze}/\", \"\") for obj in response_s3['Contents'] if obj['Key'].endswith('.csv')]\n",
    "        print(f\"üìÅ Encontrados {len(arquivos_existentes)} arquivos j√° existentes na camada bronze\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao verificar arquivos existentes: {e}\")\n",
    "        arquivos_existentes = []\n",
    "    \n",
    "    bases_ingeridas = []\n",
    "    bases_ignoradas = []\n",
    "    \n",
    "    for arquivo in arquivos_zip:\n",
    "        nome_arquivo = arquivo['name']\n",
    "        download_url = arquivo['download_url']\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîÑ Processando arquivo: {nome_arquivo}\")\n",
    "            \n",
    "            # Baixar o arquivo ZIP\n",
    "            zip_response = requests.get(download_url)\n",
    "            zip_response.raise_for_status()\n",
    "            \n",
    "            # Extrair arquivos CSV do ZIP\n",
    "            with zipfile.ZipFile(io.BytesIO(zip_response.content), 'r') as zip_file:\n",
    "                # Listar arquivos no ZIP\n",
    "                arquivos_no_zip = zip_file.namelist()\n",
    "                arquivos_csv_no_zip = [f for f in arquivos_no_zip if f.endswith('.csv')]\n",
    "                \n",
    "                print(f\"üìä Encontrados {len(arquivos_csv_no_zip)} arquivos CSV no ZIP\")\n",
    "                \n",
    "                for arquivo_csv in arquivos_csv_no_zip:\n",
    "                    nome_arquivo_csv = os.path.basename(arquivo_csv)\n",
    "                    \n",
    "                    # Verificar se o arquivo j√° existe na camada bronze\n",
    "                    if nome_arquivo_csv in arquivos_existentes:\n",
    "                        print(f\"‚è≠Ô∏è {nome_arquivo_csv} j√° existe na camada bronze - ignorando\")\n",
    "                        bases_ignoradas.append(nome_arquivo_csv)\n",
    "                        continue\n",
    "                    \n",
    "                    # Extrair conte√∫do do arquivo CSV\n",
    "                    conteudo_csv = zip_file.read(arquivo_csv)\n",
    "                    \n",
    "                    # Fazer upload para S3 bronze\n",
    "                    s3_key = f\"{s3_bronze}/{nome_arquivo_csv}\"\n",
    "                    \n",
    "                    s3_client.put_object(\n",
    "                        Bucket=s3_bucket,\n",
    "                        Key=s3_key,\n",
    "                        Body=conteudo_csv,\n",
    "                        ContentType='text/csv'\n",
    "                    )\n",
    "                    \n",
    "                    # Verificar se o arquivo foi carregado com sucesso\n",
    "                    try:\n",
    "                        s3_client.head_object(Bucket=s3_bucket, Key=s3_key)\n",
    "                        print(f\"‚úÖ {nome_arquivo_csv} extra√≠do e ingerido com sucesso na camada bronze\")\n",
    "                        bases_ingeridas.append(nome_arquivo_csv)\n",
    "                    except ClientError:\n",
    "                        print(f\"‚ùå Falha na verifica√ß√£o do arquivo {nome_arquivo_csv}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {nome_arquivo}: {e}\")\n",
    "    \n",
    "    # Mostrar resumo final\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä RESUMO DA INGEST√ÉO:\")\n",
    "    print(f\"‚úÖ Bases ingeridas: {len(bases_ingeridas)}\")\n",
    "    for base in bases_ingeridas:\n",
    "        print(f\"  üìÅ {base}\")\n",
    "    \n",
    "    if bases_ignoradas:\n",
    "        print(f\"‚è≠Ô∏è Bases ignoradas (j√° existentes): {len(bases_ignoradas)}\")\n",
    "        for base in bases_ignoradas:\n",
    "            print(f\"  üìÅ {base}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro geral na ingest√£o: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b7ec3",
   "metadata": {},
   "source": [
    "## Teste Conex√£o DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d254ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar engine de conex√£o usando as vari√°veis do .env\n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario_pg}:{senha_pg}@{host_pg}:{porta_pg}/{banco_pg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "633ebbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection(engine):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # Testa a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Lista as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"  -\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ac775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado com sucesso: PostgreSQL 17.4 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 12.4.0, 64-bit\n",
      "üìÑ Tabelas no banco:\n",
      "  - pnad_covid_092020\n",
      "  - pnad_covid_102020\n",
      "  - pnad_covid_112020\n"
     ]
    }
   ],
   "source": [
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e880fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conex√£o psycopg2 pura a partir do engine SQLAlchemy\n",
    "conn = engine.raw_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63683c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Banco de dados 'postechcovid' j√° existe\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    # Verificar se o banco de dados j√° existe\n",
    "    result = connection.execute(text(\"\"\"\n",
    "        SELECT 1 FROM pg_database WHERE datname = 'postechcovid';\n",
    "    \"\"\"))\n",
    "    \n",
    "    if result.fetchone():\n",
    "        print(\"‚ÑπÔ∏è Banco de dados 'postechcovid' j√° existe\")\n",
    "    else:\n",
    "        connection.execute(text(\"COMMIT\"))  # Necess√°rio para criar banco fora de transa√ß√£o\n",
    "        connection.execute(text(\"CREATE DATABASE postechcovid;\"))\n",
    "        print(\"‚úÖ Banco de dados 'postechcovid' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e12400",
   "metadata": {},
   "source": [
    "### Criar as tabelas no Banco de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6162091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Tabela 'pnad_covid_092020' j√° existe - pulando cria√ß√£o\n",
      "‚ÑπÔ∏è Tabela 'pnad_covid_102020' j√° existe - pulando cria√ß√£o\n",
      "‚ÑπÔ∏è Tabela 'pnad_covid_112020' j√° existe - pulando cria√ß√£o\n"
     ]
    }
   ],
   "source": [
    "sql_files = [\n",
    "    r\"E:\\Documentos\\P√≥s Tech\\Fase 3 - Big Data\\Tech Challenge\\SQL\\postgres_bronze_092020.sql\",\n",
    "    r\"E:\\Documentos\\P√≥s Tech\\Fase 3 - Big Data\\Tech Challenge\\SQL\\postgres_bronze_102020.sql\",\n",
    "    r\"E:\\Documentos\\P√≥s Tech\\Fase 3 - Big Data\\Tech Challenge\\SQL\\postgres_bronze_112020.sql\",\n",
    "]\n",
    "\n",
    "# Mapear arquivos SQL para seus nomes de tabelas correspondentes\n",
    "table_names = {\n",
    "    \"postgres_bronze_092020.sql\": \"pnad_covid_092020\",\n",
    "    \"postgres_bronze_102020.sql\": \"pnad_covid_102020\", \n",
    "    \"postgres_bronze_112020.sql\": \"pnad_covid_112020\"\n",
    "}\n",
    "\n",
    "# Executar cada arquivo SQL com valida√ß√£o\n",
    "with engine.begin() as conn:\n",
    "    for file_path in sql_files:\n",
    "        file_name = file_path.split('\\\\')[-1]  # Extrair nome do arquivo\n",
    "        table_name = table_names.get(file_name)\n",
    "        \n",
    "        if table_name:\n",
    "            # Verificar se a tabela j√° existe\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT FROM information_schema.tables \n",
    "                    WHERE table_schema = 'public' \n",
    "                    AND table_name = :table_name\n",
    "                );\n",
    "            \"\"\"), {\"table_name\": table_name})\n",
    "            \n",
    "            table_exists = result.scalar()\n",
    "            \n",
    "            if table_exists:\n",
    "                print(f\"‚ÑπÔ∏è Tabela '{table_name}' j√° existe - pulando cria√ß√£o\")\n",
    "                continue\n",
    "        \n",
    "        # Executar o arquivo SQL para criar a tabela\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            sql_content = file.read()\n",
    "            conn.execute(text(sql_content))\n",
    "            print(f\"‚úÖ Tabela criada: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "134a97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Iniciando ingest√£o otimizada via COPY (CSV -> PostgreSQL)...\n",
      "\n",
      "üìä Processando PNAD_COVID_092020.csv -> pnad_covid_092020...\n",
      "‚úÖ Dados inseridos em pnad_covid_092020 via COPY!\n",
      "\n",
      "üìä Processando PNAD_COVID_102020.csv -> pnad_covid_102020...\n",
      "‚úÖ Dados inseridos em pnad_covid_092020 via COPY!\n",
      "\n",
      "üìä Processando PNAD_COVID_102020.csv -> pnad_covid_102020...\n",
      "‚úÖ Dados inseridos em pnad_covid_102020 via COPY!\n",
      "\n",
      "üìä Processando PNAD_COVID_112020.csv -> pnad_covid_112020...\n",
      "‚úÖ Dados inseridos em pnad_covid_102020 via COPY!\n",
      "\n",
      "üìä Processando PNAD_COVID_112020.csv -> pnad_covid_112020...\n",
      "‚úÖ Dados inseridos em pnad_covid_112020 via COPY!\n",
      "\n",
      "==================================================\n",
      "üìä RESUMO DA CARGA OTIMIZADA NO POSTGRESQL (COPY):\n",
      "‚úÖ Dados inseridos em pnad_covid_112020 via COPY!\n",
      "\n",
      "==================================================\n",
      "üìä RESUMO DA CARGA OTIMIZADA NO POSTGRESQL (COPY):\n",
      "üìã pnad_covid_092020: 387,298 registros\n",
      "üìã pnad_covid_092020: 387,298 registros\n",
      "üìã pnad_covid_102020: 380,461 registros\n",
      "üìã pnad_covid_102020: 380,461 registros\n",
      "üìã pnad_covid_112020: 381,438 registros\n",
      "==================================================\n",
      "üìã pnad_covid_112020: 381,438 registros\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Ingest√£o otimizada: COPY direto do CSV do S3 para o PostgreSQL usando conex√£o j√° existente\n",
    "import io\n",
    "\n",
    "if 'conn' not in globals():\n",
    "    raise RuntimeError(\"A conex√£o 'conn' com o banco de dados precisa estar criada antes de rodar esta c√©lula.\")\n",
    "\n",
    "print(\"üîÑ Iniciando ingest√£o otimizada via COPY (CSV -> PostgreSQL)...\")\n",
    "\n",
    "mapeamento_csv_tabela = {\n",
    "    'PNAD_COVID_092020.csv': 'pnad_covid_092020',\n",
    "    'PNAD_COVID_102020.csv': 'pnad_covid_102020',\n",
    "    'PNAD_COVID_112020.csv': 'pnad_covid_112020'\n",
    "}\n",
    "\n",
    "for arquivo_csv, nome_tabela in mapeamento_csv_tabela.items():\n",
    "    try:\n",
    "        print(f\"\\nüìä Processando {arquivo_csv} -> {nome_tabela}...\")\n",
    "        s3_key_csv = f\"{s3_bronze}/{arquivo_csv}\"\n",
    "        # Baixar CSV do S3 para mem√≥ria\n",
    "        csv_obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key_csv)\n",
    "        csv_bytes = csv_obj['Body'].read()\n",
    "        csv_buffer = io.BytesIO(csv_bytes)\n",
    "\n",
    "        # Usar conex√£o j√° existente (conn)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"TRUNCATE TABLE {nome_tabela};\")\n",
    "        conn.commit()\n",
    "        cur.copy_expert(f\"COPY {nome_tabela} FROM STDIN WITH (FORMAT CSV, HEADER TRUE, DELIMITER ',')\", csv_buffer)\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Dados inseridos em {nome_tabela} via COPY!\")\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {arquivo_csv} via COPY: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä RESUMO DA CARGA OTIMIZADA NO POSTGRESQL (COPY):\")\n",
    "with engine.connect() as conn_check:\n",
    "    for arquivo_csv, nome_tabela in mapeamento_csv_tabela.items():\n",
    "        try:\n",
    "            result = conn_check.execute(text(f\"SELECT COUNT(*) FROM {nome_tabela};\"))\n",
    "            count = result.scalar()\n",
    "            print(f\"üìã {nome_tabela}: {count:,} registros\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao verificar {nome_tabela}: {e}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa093718",
   "metadata": {},
   "source": [
    "O m√©todo COPY do PostgreSQL √© uma das formas mais r√°pidas e eficientes de inserir grandes volumes de dados em uma tabela. Ele permite importar dados diretamente de um arquivo (ou stream) no formato CSV, TXT, etc., para dentro do banco, minimizando a sobrecarga de transa√ß√µes e parsing linha a linha.\n",
    "\n",
    "No seu notebook, a ingest√£o est√° sendo realizada assim:\n",
    "\n",
    "Para cada arquivo CSV no S3, o arquivo √© baixado para a mem√≥ria.\n",
    "Uma conex√£o nativa psycopg2 √© usada para acessar m√©todos avan√ßados do PostgreSQL.\n",
    "Antes de inserir, a tabela de destino √© truncada (limpa).\n",
    "O m√©todo cur.copy_expert executa o comando COPY, lendo o CSV diretamente da mem√≥ria e inserindo em lote na tabela, com suporte a cabe√ßalho e delimitador.\n",
    "Ap√≥s cada carga, um resumo √© impresso mostrando o total de registros em cada tabela.\n",
    "Esse processo garante ingest√£o r√°pida, eficiente e segura, aproveitando o m√°ximo desempenho do PostgreSQL para cargas de dados em massa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
